# what will you do if the server side crushes?

When a server-side application experiences a crash (failure), it's critical to handle the situation effectively to minimize downtime, data loss, and user impact. A comprehensive error handling and recovery strategy is essential for production applications.

## Immediate Response to Server Crash

### 1. **Detect the Crash**
   - Monitor application health using health check endpoints
   - Implement process monitoring tools (PM2, forever, systemd)
   - Use application performance monitoring (APM) tools like New Relic, Datadog, or Sentry

### 2. **Automatic Recovery**
   - **Process Managers**: Automatically restart crashed processes
   - **Load Balancers**: Redirect traffic from failed instances
   - **Container Orchestration**: Kubernetes auto-healing

## Comprehensive Crash Handling Strategy

### Process Management with PM2

```javascript
// ecosystem.config.js
module.exports = {
  apps: [{
    name: 'my-app',
    script: './app.js',
    instances: 'max', // Launch as many instances as possible
    exec_mode: 'cluster', // Cluster mode for load balancing
    env: {
      NODE_ENV: 'production',
      PORT: 3000
    },
    error_file: './logs/err.log',
    out_file: './logs/out.log',
    log_file: './logs/combined.log',
    time: true,
    // Restart policies
    restart_delay: 4000,
    max_memory_restart: '1G',
    max_restarts: 10,
    min_uptime: '10s'
  }]
};

// Commands
pm2 start ecosystem.config.js  // Start application
pm2 restart all                // Restart all processes  
pm2 reload all                 // Zero-downtime reload
pm2 stop all                   // Stop all processes
pm2 logs                       // View logs
pm2 monit                      // Monitor processes
```

### Container Orchestration (Docker + Kubernetes)

```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-app
        image: my-app:latest
        ports:
        - containerPort: 3000
        livenessProbe:
          httpGet:
            path: /health
            port: 3000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 3000
          initialDelaySeconds: 5
          periodSeconds: 5
```

## Error Handling in Code

### Uncaught Exception Handling

```javascript
process.on('uncaughtException', (error) => {
  console.error('Uncaught Exception:', error);
  
  // Log to external service
  errorLoggingService.logError({
    type: 'uncaught_exception',
    error: error.message,
    stack: error.stack,
    timestamp: new Date().toISOString()
  });
  
  // Graceful shutdown
  server.close(() => {
    console.log('HTTP server closed due to uncaught exception');
    process.exit(1);
  });
});

process.on('unhandledRejection', (reason, promise) => {
  console.error('Unhandled Rejection at:', promise, 'reason:', reason);
  
  // Log unhandled promise rejections
  errorLoggingService.logError({
    type: 'unhandled_rejection',
    reason: reason.toString(),
    promise: promise.toString()
  });
});
```

### Error Recovery Patterns

#### Circuit Breaker Pattern

```javascript
class CircuitBreaker {
  constructor(failureThreshold = 5, timeout = 60000) {
    this.failureThreshold = failureThreshold;
    this.timeout = timeout;
    this.failureCount = 0;
    this.lastFailureTime = null;
    this.state = 'CLOSED'; // CLOSED, OPEN, HALF_OPEN
  }

  async execute(operation) {
    if (this.state === 'OPEN') {
      if (Date.now() - this.lastFailureTime > this.timeout) {
        this.state = 'HALF_OPEN';
      } else {
        throw new Error('Circuit breaker is OPEN');
      }
    }

    try {
      const result = await operation();
      this.onSuccess();
      return result;
    } catch (error) {
      this.onFailure();
      throw error;
    }
  }

  onSuccess() {
    this.failureCount = 0;
    this.state = 'CLOSED';
  }

  onFailure() {
    this.failureCount++;
    this.lastFailureTime = Date.now();
    
    if (this.failureCount >= this.failureThreshold) {
      this.state = 'OPEN';
    }
  }
}

// Usage
const dbCircuitBreaker = new CircuitBreaker(3, 30000); // 3 failures, 30s timeout

app.get('/api/data', async (req, res) => {
  try {
    const data = await dbCircuitBreaker.execute(() => 
      database.query('SELECT * FROM data')
    );
    res.json(data);
  } catch (error) {
    if (error.message.includes('Circuit breaker')) {
      res.status(503).json({ error: 'Service temporarily unavailable' });
    } else {
      res.status(500).json({ error: 'Internal server error' });
    }
  }
});
```

### Graceful Shutdown

```javascript
let server;
const connections = new Set();

function gracefulShutdown(signal) {
  console.log(`Received ${signal}. Starting graceful shutdown...`);
  
  // Stop accepting new connections
  server.close(() => {
    console.log('HTTP server closed');
  });
  
  // Close existing connections
  for (const connection of connections) {
    connection.end();
  }
  
  // Close database connections
  database.disconnect();
  
  // Force exit after timeout
  setTimeout(() => {
    console.error('Forced shutdown after timeout');
    process.exit(1);
  }, 30000);
}

// Handle shutdown signals
process.on('SIGTERM', () => gracefulShutdown('SIGTERM'));
process.on('SIGINT', () => gracefulShutdown('SIGINT'));

// Track connections
server = app.listen(PORT, () => {
  console.log(`Server running on port ${PORT}`);
});

server.on('connection', (connection) => {
  connections.add(connection);
  connection.on('close', () => {
    connections.delete(connection);
  });
});
```

## Monitoring and Alerting

### Health Check Endpoints

```javascript
// Health check endpoint
app.get('/health', async (req, res) => {
  try {
    // Check database connection
    await database.ping();
    
    // Check external services
    const response = await axios.get('https://external-api.com/health');
    
    // Check memory usage
    const memoryUsage = process.memoryUsage();
    
    res.status(200).json({
      status: 'healthy',
      timestamp: new Date().toISOString(),
      uptime: process.uptime(),
      memory: memoryUsage,
      checks: {
        database: 'ok',
        external_api: response.status === 200 ? 'ok' : 'fail'
      }
    });
  } catch (error) {
    console.error('Health check failed:', error);
    res.status(503).json({
      status: 'unhealthy',
      error: error.message,
      timestamp: new Date().toISOString()
    });
  }
});

// Readiness check
app.get('/ready', (req, res) => {
  // Simple readiness check
  res.status(200).json({ status: 'ready' });
});
```

### External Monitoring Services

```javascript
// Sentry for error tracking
const Sentry = require('@sentry/node');
Sentry.init({ dsn: process.env.SENTRY_DSN });

// Winston for logging
const winston = require('winston');
const logger = winston.createLogger({
  level: 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.errors({ stack: true }),
    winston.format.json()
  ),
  transports: [
    new winston.transports.File({ filename: 'logs/error.log', level: 'error' }),
    new winston.transports.File({ filename: 'logs/combined.log' })
  ]
});

process.on('uncaughtException', (error) => {
  logger.error('Uncaught Exception:', error);
  Sentry.captureException(error);
  // Graceful shutdown...
});
```

## Database Recovery Strategies

### Connection Pooling and Retry Logic

```javascript
// Database connection with retry
const mysql = require('mysql2');

const pool = mysql.createPool({
  host: process.env.DB_HOST,
  user: process.env.DB_USER,
  password: process.env.DB_PASSWORD,
  database: process.env.DB_NAME,
  connectionLimit: 10,
  // Enable reconnection
  acquireTimeout: 60000,
  timeout: 60000,
  reconnect: true
});

// Retry wrapper for database operations
async function executeWithRetry(operation, maxRetries = 3) {
  let lastError;
  
  for (let i = 0; i < maxRetries; i++) {
    try {
      return await operation();
    } catch (error) {
      lastError = error;
      console.log(`Database operation failed (attempt ${i + 1}):`, error.message);
      
      // Wait before retry (exponential backoff)
      await new Promise(resolve => setTimeout(resolve, Math.pow(2, i) * 1000));
    }
  }
  
  throw lastError;
}

// Usage
app.get('/api/users', async (req, res) => {
  try {
    const users = await executeWithRetry(() => 
      pool.promise().query('SELECT * FROM users')
    );
    res.json(users[0]);
  } catch (error) {
    console.error('Failed to fetch users after retries:', error);
    res.status(500).json({ error: 'Database error' });
  }
});
```

## Performance Degradation Handling

### Load Shedding

```javascript
let requestCount = 0;
const MAX_CONCURRENT_REQUESTS = 100;

app.use((req, res, next) => {
  requestCount++;
  
  if (requestCount > MAX_CONCURRENT_REQUESTS) {
    return res.status(503).json({
      error: 'Server is under high load. Please try again later.'
    });
  }
  
  res.on('finish', () => {
    requestCount--;
  });
  
  next();
});

// Request timeout
app.use((req, res, next) => {
  const timeout = setTimeout(() => {
    if (!res.headersSent) {
      res.status(408).json({ error: 'Request timeout' });
    }
  }, 30000); // 30 second timeout
  
  res.on('finish', () => clearTimeout(timeout));
  next();
});
```

## Disaster Recovery

### Backup and Restore Strategies

```bash
# Database backup script
#!/bin/bash
DATE=$(date +%Y%m%d_%H%M%S)
mysqldump -u root -p mydatabase > backup_$DATE.sql

# Automated restore
mysql -u root -p mydatabase < backup_latest.sql
```

### Multi-Region Deployment

```javascript
// Multi-region health checking
const regions = ['us-west-1', 'us-east-1', 'eu-west-1'];

function checkRegionHealth(region) {
  // Check latency and availability
  return fetch(`https://${region}.api.example.com/health`)
    .then(response => response.json())
    .then(data => ({
      region,
      healthy: data.status === 'healthy',
      latency: measureLatency(region)
    }));
}

// Route traffic to healthiest region
async function getBestRegion() {
  const healthChecks = await Promise.all(regions.map(checkRegionHealth));
  return healthChecks
    .filter(check => check.healthy)
    .sort((a, b) => a.latency - b.latency)[0]?.region;
}
```

## Post-Crash Analysis

### Crash Analysis Tools

```javascript
// Core dump analysis
const fs = require('fs');

process.on('SIGUSR2', () => {
  console.log('Generating core dump...');
  process.abort(); // Generate core dump
});

// Memory leak detection
const memwatch = require('memwatch-next');

memwatch.on('leak', (info) => {
  console.error('Memory leak detected:', info);
  errorLoggingService.logError({
    type: 'memory_leak',
    info,
    timestamp: new Date().toISOString()
  });
});

memwatch.on('stats', (stats) => {
  if (stats.usage_trend > 1.2) { // 20% increase
    console.warn('High memory usage trend:', stats);
  }
});
```

## Best Practices Summary

### 1. **Implement Comprehensive Monitoring**
   - Health checks, metrics, alerting
   - Log aggregation and analysis
   - Performance monitoring

### 2. **Use Process Managers**
   - PM2, forever, or systemd for auto-restart
   - Load balancing across multiple instances
   - Resource limits and monitoring

### 3. **Build Resilient Applications**
   - Graceful error handling
   - Circuit breakers for external services
   - Retry logic with exponential backoff

### 4. **Implement Disaster Recovery**
   - Automated backups
   - Multi-region deployments
   - Clear rollback procedures

### 5. **Monitor and Alert**
   - Real-time monitoring systems
   - Automated alerting for issues
   - Regular health check validation

### 6. **Plan for Scalability**
   - Horizontal scaling capabilities
   - Load balancing configurations
   - Resource allocation strategies

## Summary

Handling server crashes effectively requires a multi-layered approach:

1. **Prevention**: Robust error handling, input validation, resource management
2. **Detection**: Comprehensive monitoring and health checks
3. **Recovery**: Automatic restarts, circuit breakers, graceful shutdowns
4. **Analysis**: Log analysis, crash dump examination, root cause analysis
5. **Improvement**: Architecture improvements, performance optimizations

A production-ready Node.js application should implement all these layers to ensure high availability and minimal downtime in the face of crashes or failures.
